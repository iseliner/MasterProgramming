##
2018-04-03 19:55:54.564421
(nodes: 200, epoch: 20, hidden layers: 1, optimizer = rmsprop, dropout = 0.2, training items = 71352, input vector length = 350, Word2Vec based on own dataset, word1234)
Training loss: [2.4026571139651494, 2.39542931584596, 2.3835333811854524, 2.3627107224599144, 2.34069369813027, 2.3105257338292797, 2.2656411616734586, 2.215724131198311, 2.151859506254627, 2.0921564785564595, 2.028155975151535, 1.9672405850958805, 1.9026279948574103, 1.8458528501067357, 1.7936684001098901, 1.742359159844902, 1.7026860520556069, 1.6590295265505888, 1.6235264972652579, 1.586404867404271]
Training acc: [0.09513398611262125, 0.09910023804399357, 0.11203610583472476, 0.1247617483493492, 0.1406968309649909, 0.15586108739872823, 0.17389842321765633, 0.19398195281499908, 0.22072261985116126, 0.24665041568914908, 0.2725922269137892, 0.29270378759665255, 0.3158005476520025, 0.3396120729779653, 0.3559255604904213, 0.373136009449167, 0.3875014090536521, 0.4040671669866475, 0.4181802960315057, 0.42984079625526445]
Test set: [1.8789715992320668, 0.3381818181818182]


## 
2018-04-28 13:41:42.579553
nodes: 200, epoch: 20, hidden layers: 1, optimizer = rmsprop, dropout = 0.2, training items = 71352, input vector length = 350, Word2Vec based on own dataset, word1234)
Training loss: [2.402618930441709, 2.3952012544137506, 2.386094928041014, 2.3683813203915944, 2.3426686767829334, 2.3002466989384867, 2.236780649569076, 2.176559767823031, 2.103533275642697, 2.031460976032908, 1.9639539092786686, 1.9003290838900366, 1.8364800504498606, 1.7822671808930064, 1.735679148210994, 1.698186138659175, 1.6585437246494306, 1.6264341870403591, 1.5836177618637413, 1.5612784370281714]
Training acc: [0.09284430310726237, 0.10042039843237045, 0.11104178696897145, 0.12484216438650515, 0.1413461680699602, 0.1613114072586808, 0.18672846391597178, 0.21132849059558598, 0.23907779590104894, 0.2673321745872802, 0.2915459692583104, 0.31197171677015473, 0.3380869626589389, 0.35675980880716396, 0.3744373641231809, 0.3886091181007598, 0.4012953640473711, 0.41407073982305553, 0.42660843391246295, 0.43610083607807265]
Test set: [1.8436603957956488, 0.32272727272727275] essay
	  [2.536889382275668, 0.12181818187236786] speech
	  [2.1902748775482177, 0.22227272729982028] essay + speech
 
##
These models are iteratively trained, saving each step of the model per 10 epochs
nodes: 200, epoch: 10, hidden layers: 1, optimizer = rmsprop, dropout = 0.2, training items = 71352, input vector length = 400, Word2Vec based on own dataset, word1234)
2018-04-29 10:49:23.834099
batch_size=65
Training loss: [2.403377785618856, 2.3965784692296066, 2.391792775418794, 2.3818466698777887, 2.363812016971871, 2.330363396727568, 2.2856115509659496, 2.2368498811329802, 2.185967749822341, 2.134106584309784]
Training acc: [0.09192329021300645, 0.09587474390670961, 0.1023812728430691, 0.11292838612432872, 0.12314868740459246, 0.14069254759612554, 0.16143025169533234, 0.1810538241746843, 0.2003357296138756, 0.22343539330196724]

2018-04-29 21:28:52.957114
batch_size=65
Training loss: [2.0816243446002356, 2.0339833513690957, 1.9795573403254205, 1.9272791722940124, 1.8869249179355212, 1.8451600207193317, 1.8056798844718058, 1.770712496636961, 1.7337605638976636, 1.7056064711592882]
Training acc: [0.2430886765817602, 0.26281623515989333, 0.28187531451701653, 0.3016325831842704, 0.3188644861109193, 0.33241232652634134, 0.34841125668979384, 0.3604587332090029, 0.37350150089356393, 0.3862471668811746]

[missing history from 3rd iteration]

2018-04-30 13:25:37.476091 (34 epochs, stagnated at epoch 33)
batch_size = 90 
Training loss: [1.4441799243677267, 1.423081550003559, 1.4190658285972817, 1.4529876750847612]
Training acc: [0.47980451353162923, 0.4865635786205696, 0.4888958281637476, 0.4820179218962006]
Test set: [1.6646763281388717, 0.44090909069234674] essay
	  [2.566318095814098, 0.15818181823600422] speech
	  [2.1154972180453213, 0.2995454545996406] essay + speech

##
This is the same LSTM as used above, but with skip-gram Word2Vec embeddings instead of cbow
2018-05-01 09:58:30.888613
epoch = 90
Training loss: [2.40047887773181, 2.3959122608198755, 2.392564973908103, 2.3874884676745225, 2.3768274859420715, 2.36210374529548, 2.3385012206018034, 2.301723695969039, 2.2553400459325252, 2.1935282083239604]
Training acc: [0.0914776380115714, 0.09676604963810168, 0.1044609857160065, 0.11034360100428069, 0.11726607249391363, 0.12754579408710132, 0.14223747713767493, 0.15701829061689762, 0.17750345789315447, 0.2040495017003958]
Test set: [2.3752777455069802, 0.10181818181818182] essay
	  [2.39861121784557, 0.09090909093618393] speech
	  [2.386944495114413, 0.09636363639072938] speech + essay
 
2018-05-01 18:56:37.121044
Training loss: [2.1334172568700973, 2.071893359097606, 2.00274807465757, 1.9440387956991654, 1.8877566527903804, 1.828721931207105, 1.7881445153479196, 1.7509108578395363, 1.7057817937314252, 1.6773143108528912]
Training acc: [0.2267332230108754, 0.24953578573570803, 0.27565103327493795, 0.29653728864328327, 0.3179137608300949, 0.33672030264322195, 0.35245184084301173, 0.3670246822427389, 0.3805576671457603, 0.38985695250664726]
Test set: [2.331484660235318, 0.18000000002709302] essay
	  [2.390050334063443, 0.1054545455087315] speech
	  [2.3607674902135676, 0.14272727278145877] speech + essay


 
2018-05-01 21:50:34.778260
Training loss: [1.6478310582780058, 1.6156126555234513, 1.5856557196912888, 1.5673934471490862]
Training acc: [0.4023946464643466, 0.41334284642194385, 0.4238008285457052, 0.43027764697716436]
Test set: [2.3472846109216863, 0.1654545454816385] essay
	  [2.5036937392841687, 0.08000000005418605] speech
	  [2.4254891751029275, 0.12272727275436575] essay + speech

##
LSTM with sequence length 200, w2v cbow len 100, 1 hidden layer with word 1,2 and 3 ngrams of essays
2018-05-02 09:17:59.320490
Training loss: [2.3967379178657593, 2.355724544109428, 2.3089546972966635, 2.2651589460835115, 2.2292284319152915, 2.1953140888952465, 2.162200838269232, 2.12043997880718, 2.07424258720887, 2.025282558267549]
Training acc: [0.09687250086840894, 0.12026907791386664, 0.14136251033370315, 0.15894505168876355, 0.17617650368864485, 0.19318325900370384, 0.2067493413532055, 0.2265928969824522, 0.24636623502089036, 0.2645386068163559]
Test set: [1.7136680494655263, 0.43363636363636365] essay
	  [3.083369384245439, 0.1472727272998203] speech
	  [2.3985187140378086, 0.29045454545454547] essay + speech
 
2018-05-02 15:29:51.746849
Training loss: [1.9721506095992947, 1.911592306562339, 1.861458844217916, 1.7994828924852906, 1.7455357922436519, 1.6905599267467992, 1.63970126666356, 1.5973416314709061, 1.5592484185998563, 1.5166732453525813]
Training acc: [0.2889182337645153, 0.3122305488140523, 0.3307259212463401, 0.35266196581601733, 0.3738115710101344, 0.3933602100836991, 0.4122066712119799, 0.42696645574985753, 0.44150154332867053, 0.4587610823318741]
Test set: [1.6779498082941229, 0.45]
 
2018-05-02 19:51:03.886067
Training loss: [1.4804671762790058, 1.4472137144211346, 1.418874682488422, 1.388183566624182, 1.3654083546240543]
Training acc: [0.4681421827059233, 0.480219647591548, 0.4903310137628791, 0.5013552115587712, 0.5089106494370537]
Test set: [1.7846640088341452, 0.46090909090909093]

##
##LSTM with sequence length 200, w2v cbow len 100, 2 hidden layers word 1,2,3 ngram of essays
2018-05-03 08:41:35.666370
Training loss: [2.394533188527503, 2.3397251869621774, 2.287761376075413, 2.2504756766459675, 2.2231025422048156, 2.1968071290432065, 2.1718571763671655, 2.150494160331332, 2.1265859838125154, 2.100404626400006]
Training acc: [0.0959034949643475, 0.1215610855117237, 0.14019689443599612, 0.15563077067264366, 0.16911259123350203, 0.1786060402070972, 0.1913435518156083, 0.20127235135336072, 0.2141924300197361, 0.22527280192533589]
Test set: [1.6258082324808294, 0.4290909090909091] essay
 
2018-05-03 14:36:41.284483
Training loss: [2.073506884164596, 2.0429195925367996, 2.01224622049492, 1.980659541015047, 1.944482165810158, 1.908684469428511, 1.869573014519149, 1.8377934147052775, 1.800473288613376, 1.7646712940125313]
Training acc: [0.23513138373050246, 0.24909068652061647, 0.263288729476549, 0.27597006820133374, 0.28820201194031714, 0.302919666356324, 0.3193085046254691, 0.33052931129886365, 0.344657135100252, 0.3592905274079627]
Test set: [1.8133230803229592, 0.45999999978325584] essay
	  [3.183873283212835, 0.16] speeech

## 
##LSTM with sequence length 200, w2v cbow len 100, 2 hidden layers word 1,2,3 ngram of essays (essays are lowercase) nodes 200, 168, 168, 11
2018-05-03 21:10:46.037825
Training loss: [2.3967146367821957, 2.3571161952005277, 2.2991267278153678, 2.274742889476619, 2.2416902221258503, 2.21784722057862, 2.1947887411366263, 2.1721338107387793, 2.150220317734236, 2.1253032216408743]
Training acc: [0.09539792680145583, 0.11423034544243722, 0.1350709937642305, 0.146404149487144, 0.16214698408053943, 0.172426872230788, 0.1827348478328146, 0.19387139333427092, 0.2031541888092896, 0.21361664352301293]
Test set: [1.7158448986573653, 0.4036363636363636]
 
2018-05-04 08:10:11.714000
Training loss: [2.3884884664011135, 2.3447429749708757, 2.312969073080462, 2.28747781371331, 2.2610322084719785, 2.236064064164041, 2.216245190549855, 2.1925380580879654, 2.173457131191337, 2.1517738340805233]
Training acc: [0.10146474620701912, 0.12104147377269857, 0.13493055810538374, 0.1465445850686149, 0.15808839409501718, 0.17023607635193105, 0.17968739451151017, 0.19085202722942557, 0.20242392380418184, 0.21266168147701775]
Test set: [2.0082691704143176, 0.30363636363636365]
 
2018-05-04 16:32:21.950167
Training loss: [2.389204000049563, 2.3466894915537613, 2.3150963941000047, 2.2821781257334375, 2.2594294510768687, 2.2375962511865373, 2.2199781287050784, 2.203130141071582, 2.1878725467370472, 2.172189442113341]
Training acc: [0.09871220772047655, 0.11976350948518175, 0.13380707305873274, 0.14862303250409012, 0.15977362184182295, 0.17106464663274698, 0.17890095480227633, 0.18756583353565304, 0.19603410208968727, 0.2033086679086397]
Test set: [1.766817456592213, 0.35454545454545455]

##LSTM with seq len 200, w2v cbow len 100 max 80000 vocab, 2 hidden layers, word 1, char 2&3 of essay, nodes 68, dropout 0.2
2018-05-07 10:11:36.420076
Training loss: [2.379415414093702, 2.3468641373602255, 2.3226679469479956, 2.3019291521214713, 2.2843942524853915, 2.266733875797125, 2.2533759120935777, 2.238533743107345, 2.2250404841955453, 2.2136712219756083]
Training acc: [0.10379578556477673, 0.12272305002016641, 0.13262858618751078, 0.1444509524711831, 0.1525314824137349, 0.1607727047366005, 0.16742995922478685, 0.17718628095076483, 0.1822710462582349, 0.18928411987291618]
Test set: [1.7296105467189442, 0.37363636374473574]
 
2018-05-07 20:04:57.947254
Training loss: [2.385873587995649, 2.352861954324884, 2.325625257233447, 2.307003571494391, 2.2857372868653205, 2.266468803493566, 2.2461157530579254, 2.2261355989196003, 2.207718822176848, 2.189171732023639]
Training acc: [0.10080469879562344, 0.12001629374775986, 0.13165840807398818, 0.140618201362522, 0.15089808990205655, 0.1590574001523917, 0.1665145322258385, 0.17856390926124394, 0.18450433688925516, 0.19416630840207597]
Test set: [1.871631531715393, 0.30909090919928117]

#First attempt at assembling the data essay, speech and ivector 1hidden layer each (68, 10, 10)(20)
2018-05-07 23:47:15.637448
Training loss: [2.3652723836898804, 2.2230612314831126, 2.1042684345895593, 2.0052524759552695, 1.9135076713562011, 1.8327215970646251, 1.7535110340335152, 1.676783523342826, 1.6122886630621824, 1.5441771627556193, 1.4841237624125048, 1.4221417986262928, 1.3674375884099441, 1.3139775281602686, 1.2569678777998143, 1.2094794101064856, 1.1495676866986535, 1.1152899155291645, 1.0589301651174372, 1.0053333170847458, 0.9596650697426362, 0.9221345334161412, 0.8771864467317407, 0.8312938593734395, 0.7862862513823943, 0.7526574420387094, 0.7114075056802143, 0.6707108424468474, 0.6361865357919173, 0.6068556993386962]
Training acc: [0.12545454855754296, 0.18572727698494088, 0.23436364217237993, 0.2745454628630118, 0.30900001001628963, 0.34181818998672747, 0.37309091687202456, 0.39990909721363677, 0.42418182373046875, 0.44972727797248147, 0.468272733065215, 0.49572727907787667, 0.5132727364518426, 0.5320909200744195, 0.5577272814783183, 0.5769090977582064, 0.5948181891441345, 0.608545463681221, 0.6311818239905618, 0.6510000056028367, 0.6619090949405323, 0.6778181856328791, 0.6965454591404308, 0.7140909121795134, 0.7290909081697464, 0.7382727266983552, 0.7562727286057038, 0.766636361208829, 0.7835454554991289, 0.7950909096002579]
Test set: [2.7245890313928776, 0.32727272738109936]

 
2018-05-08 09:13:48.768951
Training loss: [2.3389130358262498, 2.183657080476934, 2.070558359406211, 1.9945988902178677, 1.9383303026719527, 1.8883813086423007, 1.8325617597319863, 1.7931134427677502, 1.7425515591014515, 1.703462454405698, 1.6640578853000294, 1.6467375640435653, 1.6208547817577015, 1.5822363636710428, 1.5530576233430342, 1.5221963175860318, 1.4971911304647272, 1.4692026940259066, 1.4419388159838589, 1.4144457771561363]
Training acc: [0.14027273062277923, 0.2153636394712058, 0.262181821831248, 0.28218182242729445, 0.3089090937646953, 0.32136363926258954, 0.33936363778331063, 0.35863636488264256, 0.3695454535971988, 0.3826363632353869, 0.3960909082130952, 0.40354545392773367, 0.412545453526757, 0.4241818159276789, 0.4333636351877993, 0.4464545430920341, 0.4575454527681524, 0.46290908911011436, 0.48009090824560685, 0.48927272525700655]
Test set: [1.7828144333579323, 0.36363636363636365]
 
